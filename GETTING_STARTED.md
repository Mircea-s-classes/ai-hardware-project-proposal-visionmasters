# Getting Started Guide

Welcome to the Real-Time Facial Expression Recognition project! This guide will help you get started with the project without needing the physical hardware.

## ðŸ“‹ Prerequisites

- **Python 3.9+**
- **Git**
- **Kaggle Account** (for dataset download)
- **8GB RAM minimum** (16GB recommended for training)
- **GPU optional** (significantly speeds up training)

## ðŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd ai-hardware-project-proposal-visionmasters
```

### 2. Setup Python Environment

```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Download FER2013 Dataset

#### Option A: Using Kaggle API (Recommended)

```bash
# Install Kaggle
pip install kaggle

# Setup Kaggle credentials
# 1. Go to https://www.kaggle.com/account
# 2. Click "Create New API Token"
# 3. Download kaggle.json
# 4. Move to ~/.kaggle/kaggle.json (Linux/Mac) or C:\Users\<username>\.kaggle\kaggle.json (Windows)
# 5. Set permissions (Linux/Mac only)
chmod 600 ~/.kaggle/kaggle.json

# Download dataset
cd data/fer2013
kaggle datasets download -d msambare/fer2013
unzip fer2013.zip
rm fer2013.zip
cd ../..
```

#### Option B: Manual Download

1. Visit https://www.kaggle.com/datasets/msambare/fer2013
2. Click "Download"
3. Extract to `data/fer2013/`

### 4. Verify Dataset

```bash
python src/model/prepare_data.py
```

This will:
- âœ… Verify dataset structure
- ðŸ“Š Show dataset statistics
- ðŸŽ¨ Create sample visualizations

Expected output:
```
FER2013 Dataset Preparation
============================================================
ðŸ“Š Analyzing FER2013 Dataset...
============================================================

Emotion      Train      Test     Total
------------------------------------------------------------
Angry        3,995      958      4,953
...
============================================================
âœ… Dataset preparation complete!
```

### 5. Train Baseline Model

```bash
python src/model/train_baseline.py
```

This will:
- Build MobileNetV2-based model
- Train for up to 50 epochs (with early stopping)
- Save best model to `models/baseline_fp32_best.h5`
- Generate training curves

**Expected time**: 2-3 hours with GPU, 8-12 hours with CPU

### 6. Evaluate Model

```bash
python src/model/evaluate.py --model models/baseline_fp32_best.h5
```

This will:
- Test model on test set
- Generate confusion matrix
- Create per-class accuracy charts
- Show classification report

**Target accuracy**: >85%

### 7. Quantize Model

```bash
python src/model/quantize_model.py
```

This will:
- Convert FP32 model to INT8
- Compare model sizes and accuracy
- Save quantized model to `models/model_int8.tflite`

**Expected**: ~4x size reduction, <5% accuracy drop

### 8. Benchmark Performance

```bash
python benchmarks/benchmark_model.py
```

This will:
- Measure inference latency for all models
- Test face detection speed
- Generate comparison charts

## ðŸ“Š Current Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 1: Model Development           â”‚
â”‚                 (No Hardware Required)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    prepare_data.py
           â†“
    train_baseline.py  â†’  models/baseline_fp32_best.h5
           â†“
    evaluate.py  â†’  Accuracy: 85%+
           â†“
    quantize_model.py  â†’  models/model_int8.tflite
           â†“
    benchmark_model.py  â†’  Latency analysis

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 2: Hardware Integration              â”‚
â”‚         (Requires Raspberry Pi + Coral)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    Edge TPU Compiler  â†’  model_int8_edgetpu.tflite
           â†“
    Deploy to Raspberry Pi
           â†“
    inference_demo.py  â†’  Real-time demo
           â†“
    Performance testing & optimization
```

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ prepare_data.py      # Dataset verification
â”‚   â”‚   â”œâ”€â”€ train_baseline.py    # Model training
â”‚   â”‚   â”œâ”€â”€ evaluate.py          # Model evaluation
â”‚   â”‚   â””â”€â”€ quantize_model.py    # Model quantization
â”‚   â”œâ”€â”€ hardware/
â”‚   â”‚   â””â”€â”€ inference_demo.py    # Real-time demo (Pi + Coral)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ face_detection.py    # Face detection utilities
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ benchmark_model.py       # Performance benchmarking
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fer2013/                 # Dataset (download separately)
â”‚   â””â”€â”€ emotes/                  # Clash Royale emotes
â”œâ”€â”€ models/                      # Trained models (generated)
â”œâ”€â”€ results/                     # Evaluation results (generated)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Project overview
```

## ðŸŽ¯ Current Objectives (Week 2-3)

Based on your timeline, you're in **Week 2** (Nov 12-19). Here's what you should focus on:

### Week 2: Hardware Setup & Initial Model (Current)

- [x] âœ… Project structure setup
- [ ] ðŸ“¥ Download FER2013 dataset
- [ ] ðŸ‹ï¸ Train baseline FP32 model
- [ ] ðŸ“Š Evaluate model accuracy
- [ ] ðŸŽ® Prepare emote assets

**Deliverable**: Basic FP32 model with >85% accuracy

### Week 3: Model Optimization & TPU Preparation

- [ ] ðŸ”¢ Quantize model to INT8
- [ ] ðŸ“ Benchmark inference latency
- [ ] ðŸ“¦ Prepare for Edge TPU compilation
- [ ] ðŸ“‘ Prepare midterm presentation slides

**Deliverable**: Quantized INT8 model, initial performance metrics

### Week 4: Midterm Presentation

- [ ] ðŸŽ¤ Present setup, model, and early results
- [ ] ðŸ“Š Show training curves and accuracy metrics
- [ ] ðŸš§ Discuss any challenges faced

## ðŸŽ® Preparing Clash Royale Emotes

You can copy emotes from the reference repository:

```bash
# Copy emote images
mkdir -p data/emotes/images
cp clash-royale-emote-detector/images/* data/emotes/images/

# Copy emote sounds
mkdir -p data/emotes/sounds
cp clash-royale-emote-detector/sounds/* data/emotes/sounds/

# Rename to match your emotion labels
cd data/emotes/images
mv laughing.png happy.png
mv crying.png sad.png
# ... (add more mappings as needed)
```

Or find Clash Royale emote packs online.

## ðŸ§ª Testing Without Hardware

You can test most components without the Raspberry Pi:

### Test Face Detection

```bash
python src/utils/face_detection.py
```

This will open your webcam and show face detection in real-time.

### Test Model on Webcam (CPU/GPU)

Create a simple test script to use your trained model with webcam on your development machine (without Edge TPU).

## ðŸ“ Documentation Tasks

While training models, you can work on:

1. **Midterm Presentation Slides**
   - Problem statement
   - Approach and methodology
   - Model architecture
   - Initial results

2. **Update README**
   - Add results as you get them
   - Update performance table

3. **Create System Diagram**
   - Show data flow
   - Component interactions

## ðŸ› Troubleshooting

### Out of Memory During Training

```bash
# Reduce batch size in train_baseline.py
# Change from 32 to 16 or 8
batch_size = 16
```

### GPU Not Detected

```bash
# Check TensorFlow GPU support
python -c "import tensorflow as tf; print('GPU available:', tf.config.list_physical_devices('GPU'))"

# If not available, training will use CPU (slower but works)
```

### Dataset Not Found

Make sure you've extracted the dataset to the correct location:
```
data/fer2013/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ angry/
â”‚   â”œâ”€â”€ disgust/
â”‚   â”œâ”€â”€ fear/
â”‚   â”œâ”€â”€ happy/
â”‚   â”œâ”€â”€ sad/
â”‚   â”œâ”€â”€ surprise/
â”‚   â””â”€â”€ neutral/
â””â”€â”€ test/
    â””â”€â”€ (same structure)
```

## ðŸ“š Useful Resources

### Documentation
- [MobileNetV2 Paper](https://arxiv.org/abs/1801.04381)
- [TensorFlow Lite Guide](https://www.tensorflow.org/lite)
- [MediaPipe Documentation](https://google.github.io/mediapipe/)
- [Coral Edge TPU](https://coral.ai/docs/)

### Tutorials
- [Transfer Learning with TensorFlow](https://www.tensorflow.org/tutorials/images/transfer_learning)
- [Post-Training Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)
- [Face Detection with MediaPipe](https://google.github.io/mediapipe/solutions/face_detection.html)

## ðŸ†˜ Getting Help

If you encounter issues:

1. **Check the logs**: Most scripts provide detailed error messages
2. **Review documentation**: Check relevant README files
3. **Search the error**: Google the error message
4. **Ask team members**: Collaborate with your team
5. **Office hours**: Ask your professor or TA

## âœ… Next Steps

After completing the baseline model:

1. âœ… Review results and ensure >85% accuracy
2. âœ… Document findings for midterm presentation
3. âœ… Begin quantization experiments
4. âœ… Prepare slides and demo for midterm
5. â³ Wait for hardware to arrive for integration phase

## ðŸŽ¯ Success Criteria

By the end of Week 2-3, you should have:

- âœ… FP32 model trained with >85% accuracy
- âœ… Confusion matrix and evaluation metrics
- âœ… INT8 quantized model with <5% accuracy drop
- âœ… Benchmark results showing inference latency
- âœ… Midterm presentation ready
- âœ… Clear plan for hardware integration

---

**Good luck with your project! ðŸš€**

For questions or issues, check the documentation or reach out to your team.

